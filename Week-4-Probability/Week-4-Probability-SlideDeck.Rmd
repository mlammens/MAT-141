---
title: "Introduction to Probability"
author: "Matthew E. Aiello-Lammens"
output: 
  beamer_presentation:
    slide_level: 3
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

### Slide attribution

<small>
These slides were originally developed by Mine Ã‡etinkaya-Rundel of OpenIntro and
Translated from LaTeX to Google Slides by Curry W. Hilton of OpenIntro.

These slides were modified by the OpenIntro Biostats group to accomany the OpenIntro - Introduction to Statistics for Life and Biomedical Science textbook materials.

These were further modified by Matthew Aiello-Lammens for use in Pace University course MAT 141 on the PLV campus, following and shared by the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/3.0/) 
</small>

# Basic Concepts of Probability 

### Intro to Probability

People often colloquially refer to probability...

  - "What are the chances the Twins will win this weekend?" 
  - "What's the chance of rain tomorrow?"
  - "What is the chance that a patient responds to a new therapy?"

Formalizing concepts and terminology around probability is essential for better understanding probability.

### Random Experiments

A *random experiment* is an action or process that leads to one of several possible outcomes.
  
  - For example, flipping a coin leads to two possible outcomes: either heads or tails.
  
The *probability* of an outcome is the proportion of times the outcome would occur if the random phenomenon could be observed an infinite number of times.

  - If a fair coin is flipped an infinite number of times, heads would be obtained 50\% of the time.
  
### Concepts of Probability

There are several possible interpretations of probability but they (almost) completely agree on the mathematical rules probability must follow.

* $P(A)$ = Probability of event A 
* $0 \le P(A) \le 1$

### Concepts of Probability


#### Frequentist interpretation:

The probability of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times.

**Law of large numbers** states that as more observations are collected, the proportion of occurrences with a particular outcome, ${\hat{p}_n}$, converges to the probability of that outcome, $p$.


### Concepts of Probability

#### Bayesian interpretation:

* A Bayesian interprets probability as a subjective degree of belief: For the same event, two separate people could have different viewpoints and so assign different probabilities.
* Largely popularized by revolutionary advance in computational technology and methods during the last twenty years.


### Outcomes and events   

An *outcome* in a study or experiment is the observable result after conducting the experiment.

- The sum of the faces on two dice that have been rolled.  

- The response of a patient treated with an experimental therapy.

- The total volume of eggs in a clutch laid by a frog.

An *event* is a collection of outcomes.  

- The sum after rolling two dice is 7.

- 22 of 30 patients in a study have a good response to a therapy.

- The total volume of eggs in a clutch is larger than $750 \text{ mm}^3$.

### Outcomes and events   

Events can be referred to by letters. 

  - Suppose event $A$ is the event of rolling a number smaller than 3 on a die.
  \[A = \{1, 2 \} \]

### Disjoint / Mutually Exclusive Events

Two events or outcomes are called *disjoint* or *mutually exclusive* if they cannot both happen at the same time.

<!---
\begin{center}
\includegraphics[width=2.5in]{figures/disjointEvents.png}
\end{center}
--->

\begin{columns}
\begin{column}{0.3\textwidth}
    \[A = \{1, 2 \} \] 
    \[B = \{4, 6 \} \] 
    \[D = \{2, 3 \} \]
\end{column}
\begin{column}{0.7\textwidth}
\begin{center}
\includegraphics[width=2in]{figures/disjointEvents.png}
\end{center}
\end{column}
\end{columns}

### Addition Rule for Disjoint Events

If $A$ and $B$ represent two disjoint events, then the probability that either occurs is
  \[P(A \cup B) = P(A) + P(B), \]
  
The $\cup$ symbol denotes the *union* of two events; i.e., $P(A \text{ or } B)$.\footnote{Statistics uses the inclusive "or", such that $A$ or $B$ occurring means $A$, $B$, or both $A$ and $B$ occur.}

### Addition Rule for Disjoint Events


\begin{columns}
\begin{column}{0.4\textwidth}
  $P(A \cup B) = P(A) + P(B) = 2/6 + 2/6 = 4/6$
  
  \bigskip
  
  Intuitively, this makes sense; the probability of rolling either a 1, 2, 4, or 6 on a six-sided die is 4/6.

\end{column}
\begin{column}{0.6\textwidth}
\begin{center}
\includegraphics[width=2in]{figures/disjointEvents.png}
\end{center}
\end{column}
\end{columns}

<!---
As shown on the previous slide, events $A$ and $B$ are disjoint.

  - $P(A \cup B) = P(A) + P(B) = 2/6 + 2/6 = 4/6$
  
  - Intuitively, this makes sense; the probability of rolling either a 1, 2, 4, or 6 on a six-sided die is 4/6.
--->

If there are $k$ disjoint events $A_1,\dots,A_k$, then the probability that one of these outcomes will occur is $$P(A_1) + P(A_2) + \cdots + P(A_k)$$


### General Addition Rule

Suppose that we are interested in the probability of drawing a diamond or a face card out of a standard 52-card deck.

\begin{center}
\includegraphics[width=2.5in]{figures/cardsDiamondFaceVenn.png}
\end{center}

Does $P(\text{diamond or face card}) = 13/52 + 12/52$?

### General Addition Rule...

To correct the double counting of the three cards that are in both events, subtract the probability that both events occur...
\begin{align*}
P(\text{diamond or face card}) =& P(\text{diamond}) + P(\text{face card}) \\
- P(\text{diamond and face card}) \\
=& 13/52 + 12/52 - 3/52 \\
=& 22/52
\end{align*}

Thus, for any two events $A$ and $B$, the probability that either occurs is
\[P(A \cup B) = P(A) + P(B) - P(A \cap B).\]

The $\cap$ symbol denotes the *intersection* of two events; i.e., $P(A \text{ and }B)$.


### Sample Space

A *sample space* is a list of exhaustive and mutually exclusive outcomes.
  
Suppose the possible $k$ outcomes are denoted $O_1, O_2, \dots, O_k$. The sample space can be expressed as $S = \{O_1, O_2, \dots, O_k\}$.

  - For the die tossing experiment, $S = \{1, 2, 3, 4, 5, 6\}$
  
Given a sample space $S = \{O_1, O_2, \dots, O_k\}$, the sum of the probabilities of each outcome must equal 1.

\[\sum_{i=1}^{k}P(O_i)=1\]

### Complement of an Event

Let $D = \{2, 3\}$ represent the event that the outcome of a die roll is 2 or 3. 

The *complement* of $D$ represents all outcomes in the sample space that are not in $D$.

\begin{center}
\includegraphics[width=2.5in]{figures/complementOfD.png}
\end{center}

### Complement of an Event...

The complement of an event $A$ is denoted by $A^C$. 

An event and its complement are mathematically related:

\[P(A) + P(A^C) = 1 \qquad P(A) = 1 - P(A^C)\]


### Independent Events

Two events $A$ and $B$ are *independent* if the probability that both $A$ and $B$ occur equal the product of their separate probabilities.

\[P(A \cap B) = P(A)P(B) \]

\begin{center}
\begin{figure}
\includegraphics[width=1.5in]{figures/indepForRollingTwo1s.png}
\caption{A blue die and a green die are rolled. What is the probability of rolling two 1's?}
\end{figure}
\end{center}

# Conditional Probability

### Conditional Probability: Intuition

Consider height in the US population.

What is the probability that a randomly selected individual in the population is taller than 6 feet, 4 inches?

  - Suppose you learn that the individual is a professional basketball player.
  
  - Does this change the probability that the individual is taller than 6 feet, 4 inches?

### Conditional Probability: Concept

The **conditional probability** of an event $A$, given a second event $B$, is the probability of $A$ happening, knowing that the event $B$ has happened.

  - This conditional probability is denoted $P(A|B)$.
  
Toss a fair coin three times. Let $A$ be the event that *exactly* two heads occur, and $B$ the event that *at least* two heads occur.

  - $P(A|B)$ is the probability of having exactly two heads among the outcomes that have at least two heads.
  
  - Conditioning on $B$ means that the sample space consists of $\{HHH, HHT, HTH, THH\}$, all possible sets of three tosses where at least two heads occurred.
  
  - In this set of outcomes, $A$, consists of the last three, so $P(A|B) = 3/4$.
  
### Conditional Probability: Formal Definition

As long as $P(B) > 0$,
\[P(A|B) = \dfrac{P(A \cap B)}{P(B)}. \]

From the definition,
\begin{align*}
P(A|B) =& \dfrac{P(\text{at least two heads and exactly two heads})}{P(\text{at least two heads})} \\
=& \dfrac{P(\text{exactly two heads})}{P(\text{at least two heads})} \\
=& \dfrac{(3/8)}{(4/8)} = 3/4
\end{align*}

### Independence, Again...

A consequence of the definition of conditional probability:

  - If $P(A|B) = P(A)$, then $A$ and $B$ are independent; knowing $B$ offers no information about whether $A$ occurred.


### General Multiplication Rule

If $A$ and $B$ represent two outcomes or events, then
\[P(A \cap B) = P(A|B)P(B).\]

This follows from rearranging the definition of conditional probability:
\[P(A|B) = \frac{P(A \cap B)}{P(B)} \rightarrow P(A|B)P(B) = P(A \cap B)\]

Unlike the previously mentioned multiplication rule, this is valid for events that might not be independent.


### An example related to treeating chronic cocaine use

Researchers randomly assigned 72 chronic users of cocaine into three groups: desipramine (antidepressant), lithium (standard treatment for cocaine) and placebo. Results of the study are summarized below.

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& 10		& 14		& 24 \\
lithium		& 18		& 6		& 24 \\
placebo		& 20		& 4		& 24 \\
\hline
total			& 48		& 24		& 72
\end{tabular}
\end{center}

[http://www.oswego.edu/~srp/stats/2_way_tbl_1.htm](http://www.oswego.edu/~srp/stats/2_way_tbl_1.htm)

### Relapse example

**What is the probability that a patient relapsed?**

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& 10		& 14		& 24 \\
lithium		& 18		& 6		& 24 \\
placebo		& 20		& 4		& 24 \\
\hline
total			& *48*		& 24		& *72*
\end{tabular}
\end{center}

P(relapsed) = $\frac{48}{72} \approx 0.67$

*This is called the **marginal probability***


### Relapse example

**What is the probability that a patient received the antidepressant (desipramine) *and* relapsed?**

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& *10*		& 14		& 24 \\
lithium		& 18		& 6		& 24 \\
placebo		& 20		& 4		& 24 \\
\hline
total			& 48		& 24		& *72*
\end{tabular}
\end{center}

P(relapsed and desipramine) = $\frac{10}{72} \approx 0.14$

*This is called the **joint probability***


### Relapse example

**What is the probability that a patient relapsed *given* that we know they received the antidepressant (desipramine)?**

$$
P(relapse |  desipramine) = \frac{P(relapse ~ and ~ desipramine)}{P(desipramine)}
$$

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& *10*		& 14		& *24* \\
lithium		& 18		& 6		& 24 \\
placebo		& 20		& 4		& 24 \\
\hline
total			& 48		& 24		& *72*
\end{tabular}
\end{center}

P(relapsed | desipramine) = $\frac{10/72}{24/72} \approx 0.42$

*This is called the **conditional probability***

### Relapse example

**What is the probability that a patient relapsed *given* each of the three different treatments?**

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& *10*		& 14		& *24* \\
lithium		& 18		& 6		& 24 \\
placebo		& 20		& 4		& 24 \\
\hline
total			& 48		& 24		& 72
\end{tabular}
\end{center}

P(relapsed | desipramine) = $\frac{10}{24} \approx 0.42$

### Relapse example

**What is the probability that a patient relapsed *given* each of the three different treatments?**

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& 10		& 14		& 24 \\
lithium		& *18*		& 6		& *24* \\
placebo		& 20		& 4		& 24 \\
\hline
total			& 48		& 24		& 72
\end{tabular}
\end{center}

P(relapsed | desipramine) = $\frac{10}{24} \approx 0.42$  

P(relapse $|$  lithium) = $\frac{18}{24} \approx 0.75$  

### Relapse example

**What is the probability that a patient relapsed *given* each of the three different treatments?**

\small
\begin{center}
\begin{tabular}{l | c c | c}
			& 		& no 		&  \\
			& relapse	& relapse	& total \\
\hline
desipramine	& 10		& 14		& 24 \\
lithium		& 18		& 6		& 24 \\
placebo		& *20*		& 4		& *24* \\
\hline
total			& 48		& 24		& 72
\end{tabular}
\end{center}

P(relapsed | desipramine) = $\frac{10}{24} \approx 0.42$    

P(relapse $|$  lithium) = $\frac{18}{24} \approx 0.75$  

P(relapse $|$  placebo) = $\frac{20}{24} \approx 0.83$  

# Positive Predictive Value of a Diagnostic Test (Bayes' Theorem)


### Pre-natal testing for trisomy 21, 13, and 18

Some congenital disorders are caused by an additional copy of a chromosome being attached (translocated) to another chromosome during reproduction.

- Trisomy 21: Down syndrome, approximately 1 in 800 births

- Trisomy 13: Patau's syndrome,  physical and mental
disabilities, approximately 1 in 16,000 newborns

- Trisomy 18: Edward's syndrome, nearly always fatal, either
in stillbirth or infant mortality.  Occurs in about 1 in 6,000 births

Until recently, testing for these conditions consisted of screening the mother's blood for serum markers, followed by amniocentesis in women who test positive.

### Cell-free fetal DNA (cfDNA)

cfDNA consists of copies of embryo DNA present in maternal blood.

Advances in sequencing DNA provided possibility of non-invasive testing for these disorders, using only a blood sample. 

Initial testing of the technology was done using archived samples of genetic material from children whose trisomy status was known.

The results are variable, but generally very good:

- Of 1000 unborn children with the one of the disorders, approximately 980 have cfDNA that tests positive. The test has high *sensitivity*.

- Of 1000 unborn children without the disorders, approximately 995 test negative. The test has high *specificity*.

### Cell-free fetal DNA (cfDNA)...
  
The designers of a diagnostic test want the test to be accurate.

  - In other words, the test should have high sensitivity and specificity.

A family with an unborn child undergoing testing, however, wants to know the likelihood of the condition being present if the test is positive.

Suppose a child has tested positive for trisomy 21. What is the probability that the child does have the trisomy 21 condition, given the positive test result?


### Defining events in diagnostic testing

Events of interest in diagnostic testing:

- $D$ = {disease present} 

- $D^C$ = {disease absent}  

- $T^+$ = {positive test result}  

- $T^-$ = {negative test result}  

\vspace{0.5cm}


Could use $T$ and $T^C$, but $T^+$ and $T^-$ are consistent with notation in medical and public health literature.


### Characteristics of a diagnostic test

The following measures are all characteristics of a diagnostic test.

- Sensitivity = $P (T^+ | D)$

- Specificity = $P(T^- | D^C)$  

- False negative rate = $P(T^- | D)$

    - Note that $P(T^- | D) = 1 - P (T^+ | D)$, i.e., 1 - sensitivity

- False positive rate = $P(T^+ | D^C)$

    - Note that $P(T^+ | D^C) = 1 - P (T^- | D^C)$, i.e., 1 - specificity


### Positive predictive value of a test

Suppose an individual tests positive for a disease.   

The **positive predictive value (PPV)** of a diagnostic test is the probability that a person has the disease, given that he/she has tested positive for it.

- PPV = $P(D | T^+)$

The characteristics of a diagnostic test include $P(T^+|D)$, among other probabilities, but not the reverse conditional probability $P(D|T^+)$.


### Bayes' Theorem, aka Bayes' Rule

Bayes' Thorem (simplest form):

$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$


Follows directly from the definition of conditional probability by noting that $P(A) P(B|A)$ equals $P(A \text{ and } B)$:

\[P(A|B) =  \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}\]


### The denominator P(B) in Bayes' Theorem

Bayes' Theorem is seldom stated in its simplest form, because in many problems, $P(B)$ is not given directly, but is calculated using the general multiplication formula for probabilities:

Suppose $A$ and $B$ are events.  Then,
\begin{align*}
P(B) = & P(B \cap A) + P(B \cap A^C) \\
    = & P(B | A)P(A)+ P(B|A^C)P(A^C)
\end{align*}

Bayes' Theorem can be written as:

\[P(A|B) = \frac{P(A) P(B|A)}{P(B)} = \frac{P (B|A)P(A)}{P (B | A)P(A) + P (B|A^C)P(A^C)} \]

### Bayes' Theorem for diagnostic tests

\begin{align*}
  P(D|T^+) = & \dfrac{P(D \cap T^{+})}{P(T^+)} \\ 
  =& \dfrac{P(D \cap T^{+})}{P(D \cap T^{+}) + P(D^C \cap T^{+})} \\
  = & \frac{P(T^{+}|D)P(D)}{P(T^{+}|D)P(D)+
    P(T^{+}|D^{C})P(D^C)} \\
  = & \frac{\text{sensitivity} \times \text{prevalence}}{[\text{sens.} \times \text{prev.}] + [(\text{1 - specificity}) \times (\text{1 - prev.})]}  
\end{align*}

### Bayes' Theorem for diagnostic tests

\begin{align*}
  P(D|T^+) = & \frac{\text{sensitivity} \times \text{prevalence}}{[\text{sens.} \times \text{prev.}] + [(\text{1 - specificity}) \times (\text{1 - prev.})]} \\
  \bigskip
  =& \frac{980/1000 \times 1/800}{[980/1000 \times 1/800] + [(1-995/1000) \times 799/800]} \\
  \approx& 0.197
\end{align*}

  
  
### Bayes' Theorem for diagnostic tests...

\begin{center}
\begin{figure}
\includegraphics[width=3in]{figures/bayesVenn}
\end{figure}
\end{center}


\[P(D|T^+) = \dfrac{P(D \cap T^{+})}{P(T^+)} = \dfrac{P(D \cap T^{+})}{P(D \cap T^{+}) + P(D^C \cap T^{+})}\]